{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2: Train a DNN Linear Combined Regression Model + Feature Engineering\n",
    "\n",
    " 1. Define dataset metadata + input function (to read and parse the data files, + **process features**) \n",
    " 2. Create **feature columns** based on metadata + **Extended Feature Columns**\n",
    " 3. Initialise the Estimator + **Wide & Deep Columns for the combined DNN model**\n",
    " 4. Setup an experiment with **TrainSpec, EvalSepc, Serving_fn, run_config**, and **params**\n",
    " 5. Run **train_and_evaluate** experiment \n",
    " 6. Use the **SavedModel** for predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Define input function with process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "    os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "\n",
    "print(transformed_feature_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(example_proto):\n",
    "    \n",
    "    parsed_features = tf.parse_example(serialized=example_proto, features=transformed_feature_spec)\n",
    "    parsed_features.pop(KEY_COLUMN)\n",
    "    target = parsed_features.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "    return parsed_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be applied in traing and serving\n",
    "# ideally, you put this logic in preprocess_tft, to avoid transforming the records during training several times\n",
    "\n",
    "def process_features(features):\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                 num_epochs=1, \n",
    "                 batch_size=500):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    file_names = data.Dataset.list_files(files_name_pattern)\n",
    "\n",
    "    dataset = data.TFRecordDataset(filenames=file_names)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example))\n",
    "    dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Create Feature Columns with Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_and_wide_columns():\n",
    "\n",
    "    assets_dir = os.path.join(local_models_dir, TRANSFORM_ARTEFACTS_DIR, 'transform_fn/assets')\n",
    "    \n",
    "    categorical_feature_columns = {feature_name: \n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(feature_name, vocabulary_file=os.path.join(assets_dir,feature_name ))\n",
    "      for feature_name in CATEGORICAL_FEATURE_NAMES}\n",
    "    \n",
    "    is_multiple = tf.feature_column.categorical_column_with_identity('is_multiple', num_buckets=2)\n",
    "    gestation_weeks_scaled =  tf.feature_column.numeric_column('gestation_weeks_scaled')\n",
    "    mother_age_log = tf.feature_column.numeric_column('mother_age_log')\n",
    "    mother_age_normalized = tf.feature_column.numeric_column('mother_age_normalized')\n",
    "    \n",
    "    # extended feature columns\n",
    "    cigarette_use_X_alcohol_use = tf.feature_column.crossed_column(\n",
    "      [categorical_feature_columns['cigarette_use'], categorical_feature_columns['alcohol_use']], 9)\n",
    "    \n",
    "    #mother_age_bucketized = tf.feature_column.bucketized_column(mother_age, boundaries=[18, 22, 28, 32, 36, 40, 42, 45, 50])\n",
    "    mother_age_bucketized = tf.feature_column.categorical_column_with_identity('mother_age_bucketized', num_buckets=5)\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column( [mother_age_bucketized,categorical_feature_columns['mother_race']],  120)\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(mother_race_X_mother_age_bucketized, 5)\n",
    "    \n",
    "    # wide and deep columns\n",
    "    wide_columns = categorical_feature_columns.values() + [is_multiple, cigarette_use_X_alcohol_use, mother_age_bucketized, mother_race_X_mother_age_bucketized] \n",
    "    deep_columns = [mother_age_log, gestation_weeks_scaled, mother_race_X_mother_age_bucketized_embedded]\n",
    "    \n",
    "    return wide_columns, deep_columns\n",
    "\n",
    "# w,d = get_deep_and_wide_columns()\n",
    "# print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Create a DNN Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(labels, predictions):\n",
    "\n",
    "    metrics = {}\n",
    "    \n",
    "    pred_values = predictions['predictions']\n",
    "    \n",
    "    metrics['rmse'] = tf.metrics.root_mean_squared_error(\n",
    "      labels=labels,\n",
    "      predictions=pred_values)\n",
    "    \n",
    "    metrics['mae'] = tf.metrics.mean_absolute_error(\n",
    "      labels=labels,\n",
    "      predictions=pred_values)\n",
    "    \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_DNNLinearCombinedRegressor(run_config, hparams):\n",
    "  \n",
    "    wide_columns, deep_columns = get_deep_and_wide_columns()\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_columns,\n",
    "                dnn_feature_columns = deep_columns,\n",
    "                dnn_optimizer=dnn_optimizer,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, metric_fn)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Setup Local Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) RunConfig and Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs=10,\n",
    "    batch_size=500,\n",
    "    hidden_units=[32, 16],\n",
    "    max_steps=100,\n",
    "    learning_rate=0.1,\n",
    "    evaluate_after_sec=10\n",
    ")\n",
    "\n",
    "# RunConfig\n",
    "model_dir = os.path.join(local_models_dir,\"dnn_estimator\")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serving_input_fn():\n",
    "    \n",
    "    def _serving_fn():\n",
    "        \n",
    "        # get the feature_spec of raw data\n",
    "        raw_metadata = create_raw_metadata()\n",
    "        raw_placeholder_spec = raw_metadata.schema.as_batched_placeholders()\n",
    "        raw_placeholder_spec.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "        raw_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(raw_placeholder_spec)\n",
    "        raw_features, recevier_tensors, _ = raw_input_fn()\n",
    "        \n",
    "        # apply tranform_fn on raw features\n",
    "        _, transformed_features = (\n",
    "            saved_transform_io.partially_apply_saved_transform(\n",
    "                os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR,transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            raw_features)\n",
    "        )\n",
    "        \n",
    "        # apply the process_features function to transformed features\n",
    "        transformed_features = process_features(transformed_features)\n",
    "        \n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            transformed_features, raw_features)\n",
    "    \n",
    "    return _serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) TrainSpec and EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = os.path.join(local_data_dir,TRANSFORMED_DATA_DIR)+\"/train-*.tfrecords\"\n",
    "eval_data_files = os.path.join(local_data_dir,TRANSFORMED_DATA_DIR)+\"/eval-*.tfrecords\"\n",
    "\n",
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn = lambda: tfrecords_input_fn(\n",
    "    train_data_files,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    num_epochs= hparams.num_epochs,\n",
    "    batch_size = hparams.batch_size\n",
    "  ),\n",
    "  max_steps=hparams.max_steps,\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn =lambda: tfrecords_input_fn(eval_data_files),\n",
    "  exporters=[tf.estimator.LatestExporter(\n",
    "      name=\"estimate\",  # the name of the folder in which the model will be exported to under export\n",
    "      serving_input_receiver_fn=generate_serving_input_fn(),\n",
    "      exports_to_keep=1,\n",
    "      as_text=True)],\n",
    "  steps = None,\n",
    "  throttle_secs = hparams.evaluate_after_sec # evalute after each 10 training seconds!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard - Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(model_dir)\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Run train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# remove the following line of code to resume training\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "dnn_estimator = create_DNNLinearCombinedRegressor(run_config, hparams)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"\")\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "# run train and evaluate experiment\n",
    "tf.estimator.train_and_evaluate(\n",
    "  dnn_estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/babyweight/dnn_estimator/export/estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard - Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stop TensorBoard\n",
    "TensorBoard().stop(23002)\n",
    "print('stopped TensorBoard')\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Use SavedModel for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_base_dir=os.path.join(model_dir,'export/estimate')\n",
    "SAVED_MODEL_DIR=os.path.join(saved_model_base_dir, os.listdir(saved_model_base_dir)[0])\n",
    "\n",
    "def estimate_local(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=SAVED_MODEL_DIR,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)['predictions'][0][0]\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "}\n",
    "\n",
    "prediction = estimate_local(instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
