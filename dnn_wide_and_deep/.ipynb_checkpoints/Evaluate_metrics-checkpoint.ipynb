{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluate metrics from the classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                                                                                                                                                     \n",
    "from tensorflow import data\n",
    "import tensorflow_transform as tft \n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "import argparse\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "import os\n",
    "import params\n",
    "import shutil\n",
    "import dnn_estimator\n",
    "import input\n",
    "import metadata\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "TRAIN_SIZE = metadata.TRAIN_SIZE\n",
    "NUM_EPOCHS = metadata.NUM_EPOCHS\n",
    "BATCH_SIZE = metadata.BATCH_SIZE\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "EVAL_EVERY_SEC = metadata.EVAL_EVERY_SEC\n",
    "\n",
    "\n",
    "## set the run id for the analysis run\n",
    "run_id = \"16\"\n",
    "\n",
    "MODEL_NAME = 'dnn_estimator' \n",
    "model_dir = os.path.join(params.Params.MODELS_DIR, MODEL_NAME)\n",
    "model_dir = os.path.join(model_dir, run_id)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    log_step_count_steps=1000,\n",
    "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 128), ('dropout_prob', 0.2), ('embedding_size', 3), ('hidden_units', [16, 8]), ('learning_rate', 0.1), ('max_steps', 10700), ('num_epochs', 50), ('use_indicators', True), ('use_wide_columns', True)]\n",
      "\n",
      "('Model Directory:', 'models/dnn_estimator/16')\n",
      "('Dataset Size:', 27404)\n",
      "('Batch Size:', 128)\n",
      "('Steps per Epoch:', 214)\n",
      "('Total Steps:', 10700)\n",
      "creating a dnn linear combined estimator...\n",
      "\n",
      "[]\n",
      "\n",
      "[u'total_cost_bucketized', u'per_person_cost_bucketized']\n",
      "\n",
      "['month', 'year', 'week', 'new_customer', 'cabin_type', 'balcony', 'deck', 'ship', 'destination', 'departure', 'geo_city', 'geo_region']\n",
      "\n",
      "[]\n",
      "wide columns ####################################################\n",
      "_VocabularyFileCategoricalColumn(key='week', vocabulary_file='models/transform/transform_fn/assets/week', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='geo_city', vocabulary_file='models/transform/transform_fn/assets/geo_city', vocabulary_size=3391, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='cabin_type', vocabulary_file='models/transform/transform_fn/assets/cabin_type', vocabulary_size=7, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='deck', vocabulary_file='models/transform/transform_fn/assets/deck', vocabulary_size=23, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='destination', vocabulary_file='models/transform/transform_fn/assets/destination', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='departure', vocabulary_file='models/transform/transform_fn/assets/departure', vocabulary_size=19, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='month', vocabulary_file='models/transform/transform_fn/assets/month', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='new_customer', vocabulary_file='models/transform/transform_fn/assets/new_customer', vocabulary_size=2, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='year', vocabulary_file='models/transform/transform_fn/assets/year', vocabulary_size=1, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_IdentityCategoricalColumn(key=u'total_cost_bucketized', num_buckets=6, default_value=None)\n",
      "_VocabularyFileCategoricalColumn(key='ship', vocabulary_file='models/transform/transform_fn/assets/ship', vocabulary_size=26, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='geo_region', vocabulary_file='models/transform/transform_fn/assets/geo_region', vocabulary_size=180, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_IdentityCategoricalColumn(key=u'per_person_cost_bucketized', num_buckets=6, default_value=None)\n",
      "_VocabularyFileCategoricalColumn(key='balcony', vocabulary_file='models/transform/transform_fn/assets/balcony', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_CrossedColumn(keys=('destination', 'geo_city'), hash_bucket_size=100, hash_key=None)\n",
      "_CrossedColumn(keys=('destination', 'ship'), hash_bucket_size=40, hash_key=None)\n",
      "\n",
      "deep columns ####################################################\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='week', vocabulary_file='models/transform/transform_fn/assets/week', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='geo_city', vocabulary_file='models/transform/transform_fn/assets/geo_city', vocabulary_size=3391, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='cabin_type', vocabulary_file='models/transform/transform_fn/assets/cabin_type', vocabulary_size=7, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='deck', vocabulary_file='models/transform/transform_fn/assets/deck', vocabulary_size=23, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='destination', vocabulary_file='models/transform/transform_fn/assets/destination', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='departure', vocabulary_file='models/transform/transform_fn/assets/departure', vocabulary_size=19, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='month', vocabulary_file='models/transform/transform_fn/assets/month', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='new_customer', vocabulary_file='models/transform/transform_fn/assets/new_customer', vocabulary_size=2, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='year', vocabulary_file='models/transform/transform_fn/assets/year', vocabulary_size=1, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key=u'total_cost_bucketized', num_buckets=6, default_value=None))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='ship', vocabulary_file='models/transform/transform_fn/assets/ship', vocabulary_size=26, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='geo_region', vocabulary_file='models/transform/transform_fn/assets/geo_region', vocabulary_size=180, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key=u'per_person_cost_bucketized', num_buckets=6, default_value=None))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='balcony', vocabulary_file='models/transform/transform_fn/assets/balcony', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "\n",
      "############################################################################################\n",
      "# Train Measures: {'loss': 5399.215, 'accuracy_baseline': 0.6890824, 'global_step': 10700, 'recall': 0.75474733, 'auc': 0.9414727, 'micro_accuracy': 0.84847444, 'precision': 0.8549034, 'label/mean': 0.31091765, 'average_loss': 0.27014986, 'prediction/mean': 0.32953134, 'auc_precision_recall': 0.9058083, 'accuracy': 0.88391876}\n",
      "############################################################################################\n",
      "\n",
      "############################################################################################\n",
      "# Valid Measures: {'loss': 9769.779, 'accuracy_baseline': 0.7372, 'global_step': 10700, 'recall': 0.3957382, 'auc': 0.51343024, 'micro_accuracy': 0.5076901, 'precision': 0.2705515, 'label/mean': 0.2628, 'average_loss': 1.9539559, 'prediction/mean': 0.43164667, 'auc_precision_recall': 0.31360757, 'accuracy': 0.5608}\n",
      "############################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(metadata.hparams)\n",
    "print(\"\")\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)\n",
    "\n",
    "\n",
    "VALID_SIZE = 10223\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "estimator = dnn_estimator.create_estimator(run_config, metadata.hparams)\n",
    "\n",
    "train_metrics = estimator.evaluate(\n",
    "    input_fn = input.generate_tfrecords_input_fn(\n",
    "        files_name_pattern= params.Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\",\n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE),\n",
    "    steps=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_metrics))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "eval_metrics = estimator.evaluate(\n",
    "    input_fn=input.generate_tfrecords_input_fn(\n",
    "        files_name_pattern= params.Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\",\n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= VALID_SIZE),\n",
    "    steps=1\n",
    ")\n",
    "print(\"\")\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Valid Measures: {}\".format(eval_metrics))\n",
    "print(\"############################################################################################\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88391876"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87837726"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Validation Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5608"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51343024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Re-run using different parameters</h2>\n",
    "<h2>dropout_prob=0.2</2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## set the run id for the analysis run\n",
    "run_id = \"15\"\n",
    "\n",
    "MODEL_NAME = 'dnn_estimator' \n",
    "model_dir = os.path.join(params.Params.MODELS_DIR, MODEL_NAME)\n",
    "model_dir = os.path.join(model_dir, run_id)\n",
    "\n",
    "#\n",
    "# metadata.hparams.dropout_prob = 0.2\n",
    "#`\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    log_step_count_steps=1000,\n",
    "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 128), ('dropout_prob', 0.2), ('embedding_size', 3), ('hidden_units', [16, 8]), ('learning_rate', 0.1), ('max_steps', 10700), ('num_epochs', 50), ('use_indicators', True), ('use_wide_columns', True)]\n",
      "\n",
      "('Model Directory:', 'models/dnn_estimator/15')\n",
      "('Dataset Size:', 27404)\n",
      "('Batch Size:', 128)\n",
      "('Steps per Epoch:', 214)\n",
      "('Total Steps:', 10700)\n",
      "creating a dnn linear combined estimator...\n",
      "\n",
      "[]\n",
      "\n",
      "[u'total_cost_bucketized', u'per_person_cost_bucketized']\n",
      "\n",
      "['month', 'year', 'week', 'new_customer', 'cabin_type', 'balcony', 'deck', 'ship', 'destination', 'departure', 'geo_city', 'geo_region']\n",
      "\n",
      "[]\n",
      "wide columns ####################################################\n",
      "_VocabularyFileCategoricalColumn(key='week', vocabulary_file='models/transform/transform_fn/assets/week', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='geo_city', vocabulary_file='models/transform/transform_fn/assets/geo_city', vocabulary_size=3391, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='cabin_type', vocabulary_file='models/transform/transform_fn/assets/cabin_type', vocabulary_size=7, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='deck', vocabulary_file='models/transform/transform_fn/assets/deck', vocabulary_size=23, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='destination', vocabulary_file='models/transform/transform_fn/assets/destination', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='departure', vocabulary_file='models/transform/transform_fn/assets/departure', vocabulary_size=19, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='month', vocabulary_file='models/transform/transform_fn/assets/month', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='new_customer', vocabulary_file='models/transform/transform_fn/assets/new_customer', vocabulary_size=2, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='year', vocabulary_file='models/transform/transform_fn/assets/year', vocabulary_size=1, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_IdentityCategoricalColumn(key=u'total_cost_bucketized', num_buckets=6, default_value=None)\n",
      "_VocabularyFileCategoricalColumn(key='ship', vocabulary_file='models/transform/transform_fn/assets/ship', vocabulary_size=26, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_VocabularyFileCategoricalColumn(key='geo_region', vocabulary_file='models/transform/transform_fn/assets/geo_region', vocabulary_size=180, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_IdentityCategoricalColumn(key=u'per_person_cost_bucketized', num_buckets=6, default_value=None)\n",
      "_VocabularyFileCategoricalColumn(key='balcony', vocabulary_file='models/transform/transform_fn/assets/balcony', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1)\n",
      "_CrossedColumn(keys=('destination', 'geo_city'), hash_bucket_size=100, hash_key=None)\n",
      "_CrossedColumn(keys=('destination', 'ship'), hash_bucket_size=40, hash_key=None)\n",
      "\n",
      "deep columns ####################################################\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='week', vocabulary_file='models/transform/transform_fn/assets/week', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='geo_city', vocabulary_file='models/transform/transform_fn/assets/geo_city', vocabulary_size=3391, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='cabin_type', vocabulary_file='models/transform/transform_fn/assets/cabin_type', vocabulary_size=7, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='deck', vocabulary_file='models/transform/transform_fn/assets/deck', vocabulary_size=23, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='destination', vocabulary_file='models/transform/transform_fn/assets/destination', vocabulary_size=15, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='departure', vocabulary_file='models/transform/transform_fn/assets/departure', vocabulary_size=19, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='month', vocabulary_file='models/transform/transform_fn/assets/month', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='new_customer', vocabulary_file='models/transform/transform_fn/assets/new_customer', vocabulary_size=2, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='year', vocabulary_file='models/transform/transform_fn/assets/year', vocabulary_size=1, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key=u'total_cost_bucketized', num_buckets=6, default_value=None))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='ship', vocabulary_file='models/transform/transform_fn/assets/ship', vocabulary_size=26, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='geo_region', vocabulary_file='models/transform/transform_fn/assets/geo_region', vocabulary_size=180, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "_IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key=u'per_person_cost_bucketized', num_buckets=6, default_value=None))\n",
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='balcony', vocabulary_file='models/transform/transform_fn/assets/balcony', vocabulary_size=4, num_oov_buckets=0, dtype=tf.string, default_value=-1))\n",
      "\n",
      "############################################################################################\n",
      "# Train Measures: {'loss': 7577.572, 'accuracy_baseline': 0.6890824, 'global_step': 10700, 'recall': 0.4832636, 'auc': 0.87837726, 'micro_accuracy': 0.73324525, 'precision': 0.9285714, 'label/mean': 0.31091765, 'average_loss': 0.37914398, 'prediction/mean': 0.3097896, 'auc_precision_recall': 0.8176194, 'accuracy': 0.8277795}\n",
      "############################################################################################\n",
      "\n",
      "############################################################################################\n",
      "# Valid Measures: {'loss': 4891.888, 'accuracy_baseline': 0.7372, 'global_step': 10700, 'recall': 0.21461187, 'auc': 0.5391034, 'micro_accuracy': 0.5158789, 'precision': 0.29497907, 'label/mean': 0.2628, 'average_loss': 0.97837764, 'prediction/mean': 0.34971538, 'auc_precision_recall': 0.29616922, 'accuracy': 0.6588}\n",
      "############################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(metadata.hparams)\n",
    "print(\"\")\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)\n",
    "\n",
    "\n",
    "VALID_SIZE = 10223\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "estimator = dnn_estimator.create_estimator(run_config, metadata.hparams)\n",
    "\n",
    "train_metrics = estimator.evaluate(\n",
    "    input_fn = input.generate_tfrecords_input_fn(\n",
    "        files_name_pattern= params.Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\",\n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE),\n",
    "    steps=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_metrics))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "eval_metrics = estimator.evaluate(\n",
    "    input_fn=input.generate_tfrecords_input_fn(\n",
    "        files_name_pattern= params.Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\",\n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= VALID_SIZE),\n",
    "    steps=1\n",
    ")\n",
    "print(\"\")\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Valid Measures: {}\".format(eval_metrics))\n",
    "print(\"############################################################################################\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277795"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87837726"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Validation results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6588"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5391034"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
