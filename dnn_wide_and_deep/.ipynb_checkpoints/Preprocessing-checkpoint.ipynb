{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pre-processing pipeline</h2>\n",
    "<h3>Convert the input CSV files to TF records after pre-processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                                                                                                                                                                             \n",
    "from tensorflow import data\n",
    "import tensorflow_transform as tft \n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "import os\n",
    "import params\n",
    "import featurizer\n",
    "import metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transform features </h3>\n",
    "<h3>tft.string_to_int uses an analyzer to compute the unique values taken by the input strings, and then uses TensorFlow operations to convert the input strings to indices in the table of unique values.</h3>\n",
    "<h3>\n",
    "top_k: Limit the generated vocabulary to the first `top_k` elements. If set\n",
    "      to None, the full vocabulary is generated.\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 4                                                                                                                                                                                                     \n",
    "\n",
    "def preprocess(input_features):\n",
    "\n",
    "    output_features = {}\n",
    "\n",
    "    output_features[metadata.TARGET_FEATURE_NAME] = input_features[metadata.TARGET_FEATURE_NAME]\n",
    "\n",
    "    for feature_name in metadata.NUMERIC_FEATURE_NAMES:\n",
    "\n",
    "        #output_features[feature_name+\"_scaled\"] = tft.scale_to_z_score(input_features[feature_name])\n",
    "        output_features[feature_name] = tft.scale_to_z_score(input_features[feature_name])\n",
    "\n",
    "        quantiles = tft.quantiles(input_features[feature_name], num_buckets=NUM_BUCKETS, epsilon=0.01)\n",
    "        output_features[feature_name+\"_bucketized\"] = tft.apply_buckets(input_features[feature_name],\n",
    "                                                                        bucket_boundaries=quantiles)\n",
    "\n",
    "    for feature_name in metadata.CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "        tft.uniques(input_features[feature_name], vocab_filename=feature_name)\n",
    "        output_features[feature_name] = input_features[feature_name]\n",
    "\n",
    "        # sba added this\n",
    "        #output_features[feature_name+\"_integerized\"] = tft.string_to_int(input_features[feature_name],\n",
    "                                                           #vocab_filename=feature_name)\n",
    "    for feature_name in metadata.VOCAB_FEATURE_NAMES:\n",
    "\n",
    "        output_features[feature_name +\"_integerized\"] = tft.string_to_int(input_features[feature_name],top_k=metadata.VOCAB_SIZE, num_oov_buckets=metadata.OOV_SIZE, vocab_filename=feature_name)\n",
    "                                                           \n",
    "\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Handle errors during the transformation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapAndFilterErrors(beam.PTransform):\n",
    "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
    "\n",
    "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "\n",
    "    def __init__(self, fn):\n",
    "      self._fn = fn\n",
    "      # Create a counter to measure number of bad elements.\n",
    "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
    "          'carnival_example', 'bad_elements')\n",
    "\n",
    "    def process(self, element):\n",
    "      try:\n",
    "        yield self._fn(element)\n",
    "      except Exception:  # pylint: disable=broad-except\n",
    "        # Catch any exception the above call.\n",
    "        self._bad_elements_counter.inc(1)\n",
    "\n",
    "  def __init__(self, fn):\n",
    "    self._fn = fn\n",
    "\n",
    "  def expand(self, pcoll):\n",
    "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_comma_and_filter_third_column(line):\n",
    "    # to avoid namespace error with DataflowRunner the import of csv is done\n",
    "    # locacally see https://cloud.google.com/dataflow/faq#how-do-i-handle-nameerrors\n",
    "    import csv\n",
    "    cols = list(csv.reader([line], skipinitialspace=True,))[0]\n",
    "    \n",
    "    # Remove the customer number and sail date     \n",
    "    return '\\t'.join(cols[0:3] + cols[5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline(runner, options):\n",
    "\n",
    "    options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "    print(\"Source raw train data files: {}\".format(params.Params.RAW_TRAIN_DATA_FILE))\n",
    "    print(\"Source raw train data files: {}\".format(params.Params.RAW_EVAL_DATA_FILE))\n",
    "\n",
    "    print(\"Sink transformed train data files: {}\".format(params.Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX))\n",
    "    print(\"Sink transformed data files: {}\".format(params.Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX))\n",
    "    print(\"Sink transform artefacts directory: {}\".format(params.Params.TRANSFORM_ARTEFACTS_DIR))\n",
    "\n",
    "    print(\"Temporary directory: {}\".format(params.Params.TEMP_DIR))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    with beam.Pipeline(runner, options=options) as pipeline:\n",
    "        with impl.Context(params.Params.TEMP_DIR):\n",
    "\n",
    "            raw_metadata = featurizer.create_raw_metadata()\n",
    "            converter = tft_coders.csv_coder.CsvCoder(column_names=metadata.RAW_FEATURE_NAMES,\n",
    "                                                      delimiter=params.Params.RAW_DATA_DELIMITER,\n",
    "                                                      schema=raw_metadata.schema)\n",
    "\n",
    "            ###### analyze & transform train #########################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Transform training data....\")\n",
    "\n",
    "            step = 'train'\n",
    "\n",
    "            # Read raw train data from csv files\n",
    "            raw_train_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(params.Params.RAW_TRAIN_DATA_FILE)\n",
    "              | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - FixCommasAndRemoveFiledTestData'.format(step) >> beam.Map(fix_comma_and_filter_third_column)\n",
    "              | '{} - Decode CSV Data'.format(step) >> MapAndFilterErrors(converter.decode)\n",
    "\n",
    "            )\n",
    "            \n",
    "            # create a train dataset from the data and schema\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset\n",
    "                | '{} - Analyze & Transform'.format(step) >> impl.AnalyzeAndTransformDataset(preprocess)\n",
    "            )\n",
    "\n",
    "            # get data and schema separately from the transformed_train_dataset\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink\n",
    "            _ = (\n",
    "                transformed_train_data\n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=params.Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "            ###### transform eval ##################################################################\n",
    "\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Transform eval data....\")\n",
    "\n",
    "            step = 'eval'\n",
    "\n",
    "            raw_eval_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(params.Params.RAW_EVAL_DATA_FILE)\n",
    "              | '{} - Remove Empty Lines'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - FixCommasAndRemoveFiledTestData'.format(step) >> beam.Map(fix_comma_and_filter_third_column)\n",
    "              | '{} - Decode CSV Data'.format(step) >> MapAndFilterErrors(converter.decode)\n",
    "\n",
    "            )\n",
    "\n",
    "            # create a eval dataset from the data and schema\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "\n",
    "            # transform eval data based on produced transform_fn (from analyzing train_data)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn)\n",
    "                | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "            )\n",
    "\n",
    "            # get data from the transformed_eval_dataset\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "            # write transformed eval data to sink\n",
    "            _ = (\n",
    "                transformed_eval_data\n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=params.Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "\n",
    "            ###### write transformation metadata #######################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Saving transformation artefacts ....\")\n",
    "\n",
    "            # write transform_fn as tf.graph\n",
    "            _ = (\n",
    "                transform_fn\n",
    "                | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(params.Params.TRANSFORM_ARTEFACTS_DIR)\n",
    "            )\n",
    "\n",
    "    if runner=='DataflowRunner':\n",
    "        pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run the pipeline locally or on Dataflow </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "if params.Params.TRANSFORM:\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    runner = 'DirectRunner' # DirectRunner | DataflowRunner\n",
    "\n",
    "    job_name = 'preprocess-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "    print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "    print(\"\")\n",
    "\n",
    "    options = {\n",
    "        'region': 'europe-west1',\n",
    "        'staging_location': os.path.join(params.Params.DATA_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(params.Params.DATA_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': params.Params.GCP_PROJECT_ID,\n",
    "        'worker_machine_type': 'n1-standard-2',\n",
    "        'max_num_workers': 20,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'requirements.txt',\n",
    "    }\n",
    "\n",
    "    if runner == 'DirectRunner':\n",
    "\n",
    "        shutil.rmtree(params.Params.TRANSFORM_ARTEFACTS_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(params.Params.TRANSFORMED_DATA_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(params.Params.TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "        run_transformation_pipeline(runner, options)\n",
    "        print(\"Transformation is done!\")\n",
    "else:\n",
    "    print(\"Transformation was skipped!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
